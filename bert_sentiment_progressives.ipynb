{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mm0097/bert_sentiment_progressive_analysis/blob/main/bert_sentiment_progressives.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDkS3MAxHF7P"
      },
      "source": [
        "# Correlation analysis between sentiment and use of the progressive\n",
        "This jupyter notebook aims to compile a corpus from convokit reddit corpus, preprocess a part of this data to fine-tune a BERT model for sentiment analysis using vader pre-annotation. For better results, the VADER scores can be manually edited, for instance in cases of ambiguous classifications. The fine-tuned model is then used for the sentiment (intensity) analysis of an additional dataset for the analysis. An algorithm identifies and classifies progressive constructions and additionally its linguistic features. This data is then used for correlation testing using logistic regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QSjPJObIS6q"
      },
      "source": [
        "## Installing initial dependencies/packages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit"
      ],
      "metadata": {
        "id": "2MywLAkm1eIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: other packages are loaded throughout the code for specific segments. It is necessary, to restart the runtime after the following block which is needed because of version problems with numpy, scipy and convokit."
      ],
      "metadata": {
        "id": "GSZj_XyyJsm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall \"scipy==1.10.1\"\n",
        "!pip install --upgrade --force-reinstall \"numpy==1.23.5\""
      ],
      "metadata": {
        "id": "Gq4wqsIVyGHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQbzdiJxm25E"
      },
      "outputs": [],
      "source": [
        "from convokit import Corpus, download\n",
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htu5ey_MlOx4"
      },
      "source": [
        "## Corpus construction\n",
        "This segment obtains textual data from the ConvoKit Reddit corpus and creates separate dataframes for analysis, as well as for fine-tuning of the BERT model. The size for the analysis dataset is preset to 20,000 utterances from the Reddit corpus but can easily be adjusted in the first code box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_2W6uFrljLZ"
      },
      "outputs": [],
      "source": [
        "analysis_size = 20000           # Number of posts for the linguistic analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJsZ7xOAls69"
      },
      "outputs": [],
      "source": [
        "c1 = Corpus(filename=download(\"subreddit-IdiotsInCars\"))                      # Downloading the \"IdiotsInCars\" subreddit corpus and creating a dataframe from the content\n",
        "df_base_c1 = c1.get_utterances_dataframe()\n",
        "print(\"subreddit: IdiotsInCars - \"+str(len(df_base_c1))+\" Entries\")\n",
        "\n",
        "c2 = Corpus(filename=download(\"subreddit-AmItheAsshole\"))                     # Downloading the \"AmItheAsshole\" subreddit corpus and creating a dataframe from the content\n",
        "df_base_c2 = c2.get_utterances_dataframe()\n",
        "print(\"subreddit: AmItheAsshole - \"+str(len(df_base_c2))+\" Entries\")\n",
        "\n",
        "c3 = Corpus(filename=download(\"subreddit-confessions\"))                       # Downloading the \"Confessions\" subreddit corpus and creating a dataframe from the content\n",
        "df_base_c3 = c3.get_utterances_dataframe()\n",
        "print(\"subreddit: Showerthoughts - \"+str(len(df_base_c3))+\" Entries\")\n",
        "\n",
        "#combining the datasets and removing deleted and empty posts\n",
        "df_full=pd.concat([df_base_c1,df_base_c2,df_base_c3])\n",
        "df_full['text'] = df_full['text'].replace('', pd.NA)\n",
        "df_full = df_full.dropna(subset=['text']).copy()                              #dropping empty strings\n",
        "df_full['text'] = df_full['text'].replace('[deleted]', pd.NA)\n",
        "df_full = df_full.dropna(subset=['text']).copy()                              #dropping [deleted] posts\n",
        "df_full['text'] = df_full['text'].replace('[removed]', pd.NA)\n",
        "df_full = df_full.dropna(subset=['text']).copy()                              #dropping [removed] posts\n",
        "df_full['speaker'] = df_full['speaker'].replace('AutoModerator', pd.NA)\n",
        "df_full = df_full.dropna(subset=['speaker']).copy()                           #dropping posts from AutoModerators\n",
        "df_full['text'] = df_full['text'].str.replace('\\n', ' ')\n",
        "print(\"full set: \"+str(len(df_full))+\" entries\")\n",
        "\n",
        "#splitting into a dataset for analysis and one for ML fine-tuning\n",
        "analysis_df = df_full.sample(n=200000, random_state=42)                       # Randomly sample 200k rows\n",
        "finetuning_df = df_full.drop(analysis_df.index)\n",
        "finetuning_df = finetuning_df.sample(n=250000, random_state=42)               # From the remaining posts, create a dataset for fine-tuning the BERT model\n",
        "analysis_df = analysis_df.sample(n=analysis_size, random_state=42)            # Reduce the dataset for analysis to the variable set above\n",
        "\n",
        "print(\"analysis: \"+str(len(analysis_df))+\" entries\")\n",
        "print(\"training: \"+str(len(finetuning_df))+\" entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO-S5N6qEtUi"
      },
      "source": [
        "## Fine-tuning *BERT* for sentiment valence analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSFv58aLmlyw"
      },
      "source": [
        "### Preprocessing data for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding an additional column for the text without stopwords and removing stopwords"
      ],
      "metadata": {
        "id": "WhrlSlZcJFD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH814nloo6mA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "\n",
        "finetuning_df['text_without_stopwords'] = finetuning_df['text'].apply(remove_stopwords)     # Adding an additional column for the texts without stopwords\n",
        "\n",
        "# Save the updated DataFrame\n",
        "# finetuning_df.to_csv('finetuning_with_stopwords_removed.csv', sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using VADER to analyse the stopword-stripped text for sentiment valence\n",
        "Negative, positive and neutral valences, as well as an additional compound score are predicted using VADER."
      ],
      "metadata": {
        "id": "nZX9fC9kJU9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYvpbQzCCkJJ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_scores(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores\n",
        "\n",
        "finetuning_df['sentiment'] = finetuning_df['text_without_stopwords'].apply(get_sentiment_scores)\n",
        "\n",
        "finetuning_df['neg'] = finetuning_df['sentiment'].apply(lambda x: x['neg'])\n",
        "finetuning_df['pos'] = finetuning_df['sentiment'].apply(lambda x: x['pos'])\n",
        "finetuning_df['neu'] = finetuning_df['sentiment'].apply(lambda x: x['neu'])\n",
        "finetuning_df['compound'] = finetuning_df['sentiment'].apply(lambda x: x['compound'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2W9NibTm3LI"
      },
      "source": [
        "### Fine-tuning the BERT model for text sequence classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using regression head\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm.notebook import tqdm\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Load pre-annotated finetuning dataset\n",
        "df = finetuning_df\n",
        "\n",
        "# Split the dataset into training, validation, and test sets // NOTE: the test set is not used anymore in this version of the code\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "# Fill NaN values with an empty string in the 'text' column\n",
        "train_df['text'].fillna('', inplace=True)\n",
        "val_df['text'].fillna('', inplace=True)\n",
        "test_df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, encodings, df):\n",
        "        self.encodings = encodings\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor([\n",
        "            self.df.iloc[idx]['neg'],\n",
        "            self.df.iloc[idx]['pos'],\n",
        "            self.df.iloc[idx]['compound'],\n",
        "        ], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "train_dataset = RedditDataset(train_encodings, train_df)\n",
        "val_dataset = RedditDataset(val_encodings, val_df)\n",
        "test_dataset = RedditDataset(test_encodings, test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define the regression head\n",
        "class BertRegressionHead(torch.nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(BertRegressionHead, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size, 3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None): # Include token_type_ids with default value\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) # Pass token_type_ids to bert_model\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.regressor(pooled_output)\n",
        "\n",
        "# Initialize the model with the regression head\n",
        "model = BertRegressionHead(bert_model)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Setting RMSprop optimizer\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set up learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "# Set up training parameters\n",
        "epochs = 3\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stop_count = 2\n",
        "early_stop_patience = 1\n",
        "best_val_loss = float('inf')\n",
        "no_improvement_count = 0\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "log_interval = 100\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')):\n",
        "        # Separate inputs and labels\n",
        "        inputs = {key: tensor.to(device) for key, tensor in batch.items() if key != 'labels'}  # Exclude 'labels' from inputs\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(**inputs) # Pass only the expected inputs\n",
        "        loss = torch.nn.HuberLoss()(outputs, labels) # Calculate loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print or log loss and accuracy\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            avg_loss = total_loss / log_interval\n",
        "            print(f'Batch {batch_idx + 1}/{len(train_loader)} - Loss: {avg_loss:.4f}')\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "\n",
        "    # Print or log loss and accuracy after each epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Learning rate schedule step\n",
        "    scheduler.step()\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = np.array([], dtype=float).reshape(0, 3)  # Ensures 2D shape (0 rows, 3 columns)\n",
        "    all_labels = np.array([], dtype=float).reshape(0, 3)\n",
        "\n",
        "    criterion = torch.nn.HuberLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validation'):\n",
        "            inputs = {key: tensor.to(device) for key, tensor in batch.items() if key!= 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds = np.concatenate((all_preds, outputs.cpu().numpy()))\n",
        "            all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
        "\n",
        "    # Compute average validation loss\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Calculate regression metrics by dimension\n",
        "    for i in range(3):  # Iterate over 'neg', 'neu', 'pos', 'compound'\n",
        "        mae = sklearn.metrics.mean_absolute_error(all_labels[:, i], all_preds[:, i])\n",
        "        rmse = np.sqrt(sklearn.metrics.mean_squared_error(all_labels[:, i], all_preds[:, i]))\n",
        "        accuracy = sklearn.metrics.accuracy_score(all_labels[:, i], all_preds[:, i])\n",
        "        precision = sklearn.metrics.precision_score(all_labels[:, i], all_preds[:, i])\n",
        "        recall = sklearn.metrics.recall_score(all_labels[:, i], all_preds[:, i])\n",
        "        f1_score = sklearn.metrics.f1_score(all_labels[:, i], all_preds[:, i])\n",
        "        print(f'Dimension {i} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}')\n",
        "\n",
        "    # Early stopping (based on avg_val_loss)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "        if no_improvement_count >= early_stop_patience:\n",
        "            print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
        "            break\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'ft-bert-reddit-sentiment.pth')\n",
        "\n",
        "## Print KDE plot for visualization of sentiment distributions (prediction vs. 'actual' sentiment)\n",
        "\n",
        "# Separate predicted sentiment scores\n",
        "all_preds_neg = all_preds[:, 0]\n",
        "all_preds_pos = all_preds[:, 1]\n",
        "all_preds_compound = all_preds[:, 2]\n",
        "\n",
        "# Separate actual labels as well\n",
        "all_labels_neg = all_labels[:, 0]\n",
        "all_labels_pos = all_labels[:, 1]\n",
        "all_labels_compound = all_labels[:, 2]\n",
        "\n",
        "# Create the distribution plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Use kernel density estimation (KDE) plots for better visualization of overlapping distributions\n",
        "sns.kdeplot(all_labels_neg, label='Actual Negative')\n",
        "sns.kdeplot(all_preds_neg, label='Predicted Negative')\n",
        "\n",
        "sns.kdeplot(all_labels_pos, label='Actual Positive')\n",
        "sns.kdeplot(all_preds_pos, label='Predicted Positive')\n",
        "\n",
        "sns.kdeplot(all_labels_compound, label='Actual Compound')\n",
        "sns.kdeplot(all_preds_compound, label='Predicted Compound')\n",
        "\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "34YkOJdFplCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5ifjnCA6ZQK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm.notebook import tqdm\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Load pre-annotated finetuning dataset\n",
        "df = finetuning_df\n",
        "\n",
        "# Split the dataset into training, validation, and test sets // NOTE: the test set is not used anymore in this version of the code\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "# Fill NaN values with an empty string in the 'text' column\n",
        "train_df['text'].fillna('', inplace=True)\n",
        "val_df['text'].fillna('', inplace=True)\n",
        "test_df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "train_encodings = tokenizer(list(train_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "val_encodings = tokenizer(list(val_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(list(test_df['text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Create PyTorch datasets\n",
        "class RedditDataset(Dataset):\n",
        "    def __init__(self, encodings, df):\n",
        "        self.encodings = encodings\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor([\n",
        "            self.df.iloc[idx]['neg'],\n",
        "            self.df.iloc[idx]['pos'],\n",
        "            self.df.iloc[idx]['compound'],\n",
        "        ], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "# Create PyTorch data loaders\n",
        "train_dataset = RedditDataset(train_encodings, train_df)\n",
        "val_dataset = RedditDataset(val_encodings, val_df)\n",
        "test_dataset = RedditDataset(test_encodings, test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Setting RMSprop optimizer\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Set up learning rate scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "# Set up training parameters\n",
        "epochs = 3\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stop_count = 2\n",
        "early_stop_patience = 1\n",
        "best_val_loss = float('inf')\n",
        "no_improvement_count = 0\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "log_interval = 100\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')):\n",
        "        inputs = {key: tensor.to(device) for key, tensor in batch.items()}\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = torch.nn.HuberLoss()(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print or log loss and accuracy\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            avg_loss = total_loss / log_interval\n",
        "            print(f'Batch {batch_idx + 1}/{len(train_loader)} - Loss: {avg_loss:.4f}')\n",
        "            total_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "\n",
        "    # Print or log loss and accuracy after each epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Learning rate schedule step\n",
        "    scheduler.step()\n",
        "    # Evaluate the model on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = np.array([], dtype=float).reshape(0, 3)  # Ensures 2D shape (0 rows, 3 columns)\n",
        "    all_labels = np.array([], dtype=float).reshape(0, 3)\n",
        "\n",
        "    criterion = torch.nn.HuberLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc='Validation'):\n",
        "            inputs = {key: tensor.to(device) for key, tensor in batch.items()}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            all_preds = np.concatenate((all_preds, outputs.logits.cpu().numpy()))\n",
        "            all_labels = np.concatenate((all_labels, labels.cpu().numpy()))\n",
        "\n",
        "    # Compute average validation loss\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # Calculate regression metrics by dimension\n",
        "    for i in range(3):  # Iterate over 'neg', 'neu', 'pos', 'compound'\n",
        "        mae = sklearn.metrics.mean_absolute_error(all_labels[:, i], all_preds[:, i])\n",
        "        rmse = np.sqrt(sklearn.metrics.mean_squared_error(all_labels[:, i], all_preds[:, i]))\n",
        "        print(f'Dimension {i} - MAE: {mae:.4f}, RMSE: {rmse:.4f}')\n",
        "\n",
        "    # Early stopping (based on avg_val_loss)\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improvement_count = 0\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "        if no_improvement_count >= early_stop_patience:\n",
        "            print(f'Early stopping after {epoch + 1} epochs without improvement.')\n",
        "            break\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'ft-bert-reddit-sentiment.pth')\n",
        "\n",
        "\n",
        "\n",
        "## Print KDE plot for visualization of sentiment distributions (prediction vs. 'actual' sentiment)\n",
        "\n",
        "# Separate predicted sentiment scores\n",
        "all_preds_neg = all_preds[:, 0]\n",
        "all_preds_pos = all_preds[:, 1]\n",
        "all_preds_compound = all_preds[:, 2]\n",
        "\n",
        "# Separate actual labels as well\n",
        "all_labels_neg = all_labels[:, 0]\n",
        "all_labels_pos = all_labels[:, 1]\n",
        "all_labels_compound = all_labels[:, 2]\n",
        "\n",
        "# Create the distribution plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Use kernel density estimation (KDE) plots for better visualization of overlapping distributions\n",
        "sns.kdeplot(all_labels_neg, label='Actual Negative')\n",
        "sns.kdeplot(all_preds_neg, label='Predicted Negative')\n",
        "\n",
        "sns.kdeplot(all_labels_pos, label='Actual Positive')\n",
        "sns.kdeplot(all_preds_pos, label='Predicted Positive')\n",
        "\n",
        "sns.kdeplot(all_labels_compound, label='Actual Compound')\n",
        "sns.kdeplot(all_preds_compound, label='Predicted Compound')\n",
        "\n",
        "plt.title('Distribution of Sentiment Scores')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV3N_pcoGHEZ"
      },
      "source": [
        "## Sentiment valence classification for analysis\n",
        "This segment uses the fine-tuned bert model to analyse the reddit posts for sentiment valence using three separate scores: 'pos', 'neg', and 'compound'. After the sentiment analysis, the dataset is split into sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOoykVo-YxuH"
      },
      "source": [
        "### Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to the fine-tuned model weights\n",
        "\n",
        "ft_model_path = 'ft-bert-reddit-sentiment.pth' #change for the actual path of the fine-tuned model weights"
      ],
      "metadata": {
        "id": "QU9jorqYHozn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4JQc6v0Gz30"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model\n",
        "# Create an instance of the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load fine-tuned model weights // NOTE: the path needs to be adapted, as it accesses the model directly from my Google Drive\n",
        "model.load_state_dict(torch.load(ft_model_path))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load Reddit dataset\n",
        "df = analysis_df\n",
        "\n",
        "# Preprocessing\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Applies tokenization and any other preprocessing steps used during training\"\"\"\n",
        "    tokenized_input = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "    return tokenized_input\n",
        "\n",
        "# Prediction function\n",
        "def predict_sentiment(df):\n",
        "    predictions = []\n",
        "    for utterance in df['text']:\n",
        "        inputs = preprocess_text(utterance)\n",
        "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        predictions.append(outputs.logits.cpu().numpy())\n",
        "\n",
        "    # Add predictions as new columns directly to the DataFrame\n",
        "    df['neg'] = [pred[0][0] for pred in predictions]\n",
        "    df['pos'] = [pred[0][1] for pred in predictions]\n",
        "    df['compound'] = [pred[0][2] for pred in predictions]\n",
        "    return df\n",
        "\n",
        "# Apply the prediction function to your DataFrame\n",
        "df_with_predictions = predict_sentiment(df.copy())\n",
        "\n",
        "# Save or use the predictions as needed\n",
        "df_with_predictions.head(50)\n",
        "#df_with_predictions.to_csv('reddit_sentiment_predictions.csv', index=False, sep=\"\\t\") # this line is commented out as it forwards the df to the next part without saving as csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7LrYCqY0_x"
      },
      "source": [
        "### Split into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TubX6lorR1sn"
      },
      "outputs": [],
      "source": [
        "df_with_predictions.columns=['timestamp',\n",
        " 'text',\n",
        " 'speaker',\n",
        " 'reply_to',\n",
        " 'conversation_id',\n",
        " 'meta_score',\n",
        " 'meta_top_level_comment',\n",
        " 'meta_retrieved_on',\n",
        " 'meta_gilded',\n",
        " 'meta_gildings',\n",
        " 'meta_subreddit',\n",
        " 'meta_stickied',\n",
        " 'meta_permalink',\n",
        " 'meta_author_flair_text',\n",
        " 'vectors',\n",
        " 'neg',\n",
        " 'pos',\n",
        " 'compound']\n",
        "list(df_with_predictions.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgd45zaSaoZ-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "def split_sentences_with_sentiment(df):\n",
        "    sentences = []\n",
        "    sentence_ids = []\n",
        "    utterance_ids = []\n",
        "    neg_scores = []\n",
        "    pos_scores = []\n",
        "    compound_scores = []\n",
        "    subreddits = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['text']\n",
        "        for sent_idx, sentence in enumerate(sent_tokenize(text)):\n",
        "            sentences.append(sentence)\n",
        "            sentence_ids.append(f\"{index}_{sent_idx}\")\n",
        "            utterance_ids.append(index)\n",
        "            neg_scores.append(row['neg'])  # Copy 'neg' score\n",
        "            pos_scores.append(row['pos'])  # Copy 'pos' score\n",
        "            compound_scores.append(row['compound'])  # Copy 'compound' score\n",
        "            subreddits.append(row['meta_subreddit']) # Copy subreddit\n",
        "\n",
        "    new_df = pd.DataFrame({'sentence': sentences,\n",
        "                           'sentence_id': sentence_ids,\n",
        "                           'utterance_id': utterance_ids,\n",
        "                           'subreddit': subreddits,\n",
        "                           'neg_score': neg_scores,\n",
        "                           'pos_score': pos_scores,\n",
        "                           'compound_score': compound_scores})\n",
        "    return new_df\n",
        "\n",
        "df_sentences_sentiment = split_sentences_with_sentiment(df_with_predictions)\n",
        "df_sentences_sentiment.to_csv('analysis_sentences_sentiment.csv', sep='\\t')\n",
        "print(len(df_sentences_sentiment))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_wk2InQHZyD"
      },
      "source": [
        "## Extraction of Linguistic Features\n",
        "This part of the code iterates through the sentences dataset and identifies progressives, also extracting additional linguistic features such as verb category (action, state, mental, behavioural) and stores this data in the dataframe for later analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6mFqUWYn9wp"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# nltk dependencies\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the SpaCy English model\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"ner\", \"lemmatizer\"])\n",
        "\n",
        "# using WordNet lexnames as described in the methodology section to classify present participles\n",
        "def categorize_verb(verb):\n",
        "    lemma = WordNetLemmatizer().lemmatize(verb, pos='v')\n",
        "    synsets = wn.synsets(lemma, pos=wn.VERB)\n",
        "    if synsets:\n",
        "        for synset in synsets:\n",
        "            category = synset.lexname().split('.')[1]\n",
        "            if category in ['motion', 'competition', 'change', 'communication']:\n",
        "                return \"action\"\n",
        "            elif category == 'stative':\n",
        "                return \"stative\"\n",
        "            elif category in ['cognition', 'perception']:\n",
        "                return \"private\"\n",
        "    return \"other\"\n",
        "\n",
        "# using SpaCy depency tree to distinguis main and subordinate clauses\n",
        "def identify_clause(sentence, index):\n",
        "    doc = nlp(sentence)\n",
        "    token = doc[index]\n",
        "    clause_type = \"main_clause\"\n",
        "    while token.head != token:\n",
        "        if token.dep_ in ('ccomp', 'xcomp', 'advcl', 'relcl', 'conj'):\n",
        "            clause_type = \"subclause\"\n",
        "            break\n",
        "        token = token.head\n",
        "    clause = ' '.join([tok.text_with_ws for tok in token.subtree]).strip()\n",
        "    return clause_type, clause\n",
        "\n",
        "# using a dictionary to destinguish present and past tense for the auxilliary verb\n",
        "def get_tense(verb):\n",
        "    tenses = {\n",
        "        \"is\": \"present\", \"am\": \"present\", \"are\": \"present\",\n",
        "        \"was\": \"past\", \"were\": \"past\",\n",
        "        \"being\": \"present\",\n",
        "        \"been\": \"past\"\n",
        "    }\n",
        "    return tenses.get(verb.lower(), \"unknown\")\n",
        "\n",
        "# progressive identification algorithm\n",
        "def identify_progressives(df):\n",
        "    # initialising varibales for results\n",
        "    df['has_progressive'] = 0\n",
        "    df['form_of_to_be'] = ''\n",
        "    df['intervening_words'] = ''\n",
        "    df['progressive_verb'] = ''\n",
        "    df['progressive_category'] = ''\n",
        "    df['clause_type'] = ''\n",
        "    df['tense'] = ''\n",
        "\n",
        "    for index, row in df.iterrows():        # iterating through data frame, \"read\" and tokenize text into words and performing POS tagging\n",
        "        text = str(row['sentence'])\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.text for token in doc]\n",
        "        tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "        progressive_count = 0               # initialising count variable (helper) for binary classification (has progressive or not)\n",
        "        for i, (token, tag) in enumerate(tagged_tokens):\n",
        "            if tag.startswith('VB') and token.lower() in [\"is\", \"am\", \"are\", \"was\", \"were\", \"being\", \"been\"]:     # identifying auxilliary form of to be\n",
        "                has_neighbor = False                                                                              # helper in case this is end of sequence\n",
        "                intervening_words = []                                                                            # initialising list for invervening words\n",
        "                aux_token = doc[i]                                                                                # get the corresponding SpaCy token\n",
        "                tense = get_tense(token)                                                                          # calling function to get tense of (to) be\n",
        "                aux_clause_type, aux_clause = identify_clause(text, i)                                            # identify the clause for the auxiliary verb\n",
        "\n",
        "                for j in range(i + 1, min(i + 4, len(tagged_tokens))):                                            # setting window for intervening words\n",
        "                    next_token, next_tag = tagged_tokens[j]\n",
        "                    if next_tag in [\"RB\", \"RBR\", \"RBS\", \"WRB\", \"MD\"]:                                             # storing adverbs and modals\n",
        "                        intervening_words.append(next_token)\n",
        "                    elif next_tag == \"VBG\":\n",
        "                        if i > 0 and tagged_tokens[i - 1][0].lower() != \"to\":\n",
        "                            if next_token.lower() == \"going\" and j + 2 < len(tagged_tokens) and tagged_tokens[j + 1][0].lower() == \"to\" and tagged_tokens[j + 2][1].startswith(\"VB\"):\n",
        "                                has_neighbor = False\n",
        "                                break                                                                              # excluding instances of going to future (VBG followed by \"to\"+VB)\n",
        "                            else:\n",
        "                                vbg_clause_type, vbg_clause = identify_clause(text, j)\n",
        "                                if vbg_clause == aux_clause:                                                       # if not going-to future: check whether components are in same clause\n",
        "                                    prev_word, prev_tag = tagged_tokens[i - 1]\n",
        "                                    if prev_tag not in [\"IN\", \"TO\", \"MD\"]:                                         # if \"proper\" progressive: store data\n",
        "                                        df.at[index, 'form_of_to_be'] = token\n",
        "                                        df.at[index, 'intervening_words'] = ' '.join(intervening_words)\n",
        "                                        df.at[index, 'progressive_verb'] = next_token\n",
        "                                        df.at[index, 'progressive_category'] = categorize_verb(next_token)\n",
        "                                        df.at[index, 'clause_type'] = aux_clause_type\n",
        "                                        df.at[index, 'tense'] = tense\n",
        "                                        progressive_count += 1\n",
        "                                        print(f\"Progressive found: {token} {' '.join(intervening_words)} {next_token}; \"\n",
        "                                              f\"Category: {categorize_verb(next_token)}; \"\n",
        "                                              f\"Clause Type: {aux_clause_type}; Tense: {tense}\")\n",
        "                                        has_neighbor = True\n",
        "                                        break\n",
        "\n",
        "        df.at[index, 'has_progressive'] = 1 if progressive_count > 0 else 0                                       # turn count into binary var for logistic regression\n",
        "\n",
        "    return df                                                                                                     # return updated data frame\n",
        "\n",
        "df_sentences_sentiment = identify_progressives(df_sentences_sentiment.copy())\n",
        "#df_sentences_sentiment.to_csv('all_progressive_forms.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCN9qnziiDFO"
      },
      "source": [
        "## Create Visualizations\n",
        "The following code creates histograms of the sentiment distribution in the overall sentences dataset, as well as the dataset containing only sentences with progressives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKQP3CnEiCH_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\n",
        "\n",
        "# Data for sentences with progressives\n",
        "df_prog = df_sentences_sentiment.query('has_progressive > 0')\n",
        "\n",
        "# Data for all sentences\n",
        "df_all = df_sentences_sentiment\n",
        "\n",
        "# --- Upper Row: All Sentences ---\n",
        "axes[0, 0].hist(df_all['neg_score'])\n",
        "axes[0, 0].set_title('Negative Sentiment Distribution')\n",
        "yaxis_limits = axes[0, 0].get_ylim()  # Store limits\n",
        "\n",
        "axes[0, 1].hist(df_all['pos_score'])\n",
        "axes[0, 1].set_title('Positive Sentiment Distribution')\n",
        "axes[0, 1].set_ylim(yaxis_limits)  # Enforce common limits\n",
        "\n",
        "axes[0, 2].hist(df_all['compound_score'])\n",
        "axes[0, 2].set_title('Compound Sentiment Distribution')\n",
        "axes[0, 2].set_ylim(yaxis_limits)  # Enforce common limits\n",
        "\n",
        "fig.suptitle('Distribution of Sentiment Scores', fontsize=16)\n",
        "\n",
        "# --- Lower Row: Sentences with Progressives ---\n",
        "# Repeat the process with new limits\n",
        "axes[1, 0].hist(df_prog['neg_score'])\n",
        "axes[1, 0].set_title('Negative Sentiment Distribution')\n",
        "yaxis_limits_prog = axes[1, 0].get_ylim()\n",
        "\n",
        "axes[1, 1].hist(df_prog['pos_score'])\n",
        "axes[1, 1].set_title('Positive Sentiment Distribution')\n",
        "axes[1, 1].set_ylim(yaxis_limits_prog)\n",
        "\n",
        "axes[1, 2].hist(df_prog['compound_score'])\n",
        "axes[1, 2].set_title('Compound Sentiment Distribution')\n",
        "axes[1, 2].set_ylim(yaxis_limits_prog)\n",
        "\n",
        "fig.text(-0.001, 0.75, 'All Sentences', ha='center', va='center', rotation='vertical', fontsize=14)\n",
        "fig.text(-0.001, 0.25, 'Sentences with Progressives', ha='center', va='center', rotation='vertical', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSDw0_cItZ2"
      },
      "source": [
        "## Statistical testing\n",
        "The following code uses the prepared datasets to perform logistic regression over sentiment valence and use of the progressive, as well as verb types used in the progressive constructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzG5yUjjNLDk"
      },
      "source": [
        "### General correlation of sentiment and use of the progressive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGUYdk8-jPdS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming you have your data in a pandas dataframe called \"df\"\n",
        "df = df_sentences_sentiment\n",
        "\n",
        "binary_has_progressive = df['has_progressive'].astype(int)  # Ensure it's a True/False boolean\n",
        "neg_score = df['neg_score']\n",
        "pos_score = df['pos_score']\n",
        "compound_score = df['compound_score']\n",
        "\n",
        "scores = [neg_score, pos_score, compound_score]\n",
        "\n",
        "for score in scores:\n",
        "  X = sm.add_constant(score)  # Add a constant for the intercept term\n",
        "  model = sm.Logit(binary_has_progressive, X)\n",
        "  result = model.fit()\n",
        "  print(result.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPfsw2WgNRCN"
      },
      "source": [
        "### Creation of dummy variables and testing for correlations of sentiment and verb category"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_progressive_category_dummies(df):\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create dummy variables for the 'progressive_category' column\n",
        "    progressive_category_dummies = pd.get_dummies(df_copy['progressive_category'], prefix='category', drop_first=True)\n",
        "\n",
        "    # Concatenate the dummy variables with the original DataFrame\n",
        "    df_with_dummies = pd.concat([df_copy, progressive_category_dummies], axis=1)\n",
        "\n",
        "    # Replace NaN values in the dummy variable columns with 0\n",
        "    df_with_dummies = df_with_dummies.fillna(0)\n",
        "\n",
        "    return df_with_dummies\n",
        "\n",
        "\n",
        "def create_progressive_clause_dummies(df):\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create dummy variables for the 'progressive_category' column\n",
        "    clause_dummies = pd.get_dummies(df_copy['clause_type'], prefix='cl', drop_first=True)\n",
        "\n",
        "    # Concatenate the dummy variables with the original DataFrame\n",
        "    df_with_dummies = pd.concat([df_copy, clause_dummies], axis=1)\n",
        "\n",
        "    # Replace NaN values in the dummy variable columns with 0\n",
        "    df_with_dummies = df_with_dummies.fillna(0)\n",
        "\n",
        "    return df_with_dummies\n",
        "\n",
        "def create_tense_dummies(df):\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create dummy variables for the 'progressive_category' column\n",
        "    clause_dummies = pd.get_dummies(df_copy['tense'], prefix='t', drop_first=True)\n",
        "\n",
        "    # Concatenate the dummy variables with the original DataFrame\n",
        "    df_with_dummies = pd.concat([df_copy, clause_dummies], axis=1)\n",
        "\n",
        "    # Replace NaN values in the dummy variable columns with 0\n",
        "    df_with_dummies = df_with_dummies.fillna(0)\n",
        "\n",
        "    return df_with_dummies\n",
        "\n",
        "\n",
        "# Calling the function to create dummy variables\n",
        "df = create_progressive_category_dummies(df_sentences_sentiment)\n",
        "dfc= create_progressive_clause_dummies(df)\n",
        "dfc= create_tense_dummies(dfc)"
      ],
      "metadata": {
        "id": "tfyXW4z1IWNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Za0QCShAOet"
      },
      "outputs": [],
      "source": [
        "binary_action = dfc['category_action'].astype(int)\n",
        "binary_private = dfc['category_private'].astype(int)\n",
        "binary_stative = dfc['category_stative'].astype(int)\n",
        "neg_score = dfc['neg_score']\n",
        "pos_score = dfc['pos_score']\n",
        "compound_score = dfc['compound_score']\n",
        "\n",
        "scores = [neg_score, pos_score, compound_score]\n",
        "deps = [binary_action, binary_private, binary_stative]\n",
        "\n",
        "for score in scores:\n",
        "  for dep in deps:\n",
        "    X = sm.add_constant(score)  # Add a constant for the intercept term\n",
        "    model = sm.Logit(dep, X)\n",
        "    result = model.fit()\n",
        "    print(result.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfcf=dfc.query('has_progressive>0')\n",
        "binary_presentf = dfc['t_present'].astype(int)\n",
        "binary_pastf = dfc['t_past'].astype(int)\n",
        "neg_scoref = dfcf['neg_score']\n",
        "pos_scoref = dfcf['pos_score']\n",
        "compound_scoref = dfcf['compound_score']\n",
        "\n",
        "scores = [neg_score, pos_score, compound_score]\n",
        "deps = [binary_presentf, binary_pastf]\n",
        "\n",
        "for score in scores:\n",
        "  for dep in deps:\n",
        "    X = sm.add_constant(score)  # Add a constant for the intercept term\n",
        "    model = sm.Logit(dep, X)\n",
        "    result = model.fit()\n",
        "    print(result.summary())\n"
      ],
      "metadata": {
        "id": "msBh3e8JE-Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1q8g9IMEXKq"
      },
      "outputs": [],
      "source": [
        "dfcf=dfc.query('has_progressive>0')\n",
        "binary_mainf = dfc['cl_main_clause'].astype(int)\n",
        "binary_subf = dfc['cl_subclause'].astype(int)\n",
        "neg_scoref = dfcf['neg_score']\n",
        "pos_scoref = dfcf['pos_score']\n",
        "compound_scoref = dfcf['compound_score']\n",
        "\n",
        "scores = [neg_score, pos_score, compound_score]\n",
        "deps = [binary_mainf, binary_subf]\n",
        "\n",
        "for score in scores:\n",
        "  for dep in deps:\n",
        "    X = sm.add_constant(score)  # Add a constant for the intercept term\n",
        "    model = sm.Logit(dep, X)\n",
        "    result = model.fit()\n",
        "    print(result.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnLkL9m2NZ2r"
      },
      "source": [
        "### Additional statistical info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V7rUUSLKbRz"
      },
      "outputs": [],
      "source": [
        "counts = dfc[['category_action', 'category_stative', 'category_private', 'category_other']].sum()\n",
        "df_filt = dfc.query('has_progressive > 0')\n",
        "print(\"Sentences with progressives:\")\n",
        "print(len(df_filt))\n",
        "print(\"Counts for each column:\")\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut0cSUms8HMJ"
      },
      "outputs": [],
      "source": [
        "df_action=dfc.query('category_action>0')\n",
        "df_states=dfc.query('category_stative>0')\n",
        "df_mental=dfc.query('category_private>0')\n",
        "df_uncat=dfc.query('category_other>0')\n",
        "\n",
        "print(\"action mean \\n\", df_action['compound_score'].mean())\n",
        "print(\"stative mean\\n\", df_states['compound_score'].mean())\n",
        "print(\"private mean\\n\", df_mental['compound_score'].mean())\n",
        "print(\"uncat mean\\n\", df_uncat['compound_score'].mean())\n",
        "print(\"reference\\n\", df['compound_score'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X0u1wMf4Gj3"
      },
      "outputs": [],
      "source": [
        "pc_action=len(df_action)/len(df_filt)\n",
        "pc_states=len(df_states)/len(df_filt)\n",
        "pc_mental=len(df_mental)/len(df_filt)\n",
        "pc_others=len(df_uncat)/len(df_filt)\n",
        "\n",
        "print(\"percentages\\n action: \",pc_action,\"\\n stative: \",pc_states,\"\\n private: \",pc_mental,\"\\n others: \", pc_others)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QhNZBtKEsFz"
      },
      "outputs": [],
      "source": [
        "df_main=dfc.query('cl_main_clause>0')\n",
        "df_sub=dfc.query('cl_subclause>0')\n",
        "\n",
        "pc_main=len(df_main)/len(df_filt)\n",
        "pc_sub=len(df_sub)/len(df_filt)\n",
        "\n",
        "print(\"main clause: \",pc_main)\n",
        "print(\"sub clause: \",pc_sub)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9QSjPJObIS6q",
        "tO-S5N6qEtUi",
        "SOoykVo-YxuH",
        "fw7LrYCqY0_x",
        "GCN9qnziiDFO",
        "RSSDw0_cItZ2",
        "EASb4xS9bu7_"
      ],
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1Rc2CNyQGI2X4NwuE8Mp23HDix4QbEls0",
      "authorship_tag": "ABX9TyNBJnAPGmIyy5pwTbbS6zkV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}